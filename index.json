[{"authors":["admin"],"categories":null,"content":"Benjamin Heinzerling is a postdoctoral researcher at RIKEN AIP and Tohoku University in Sendai, Japan. His research interests include analysis of entities mentioned in text, linking texts and knowledge bases, and multilingual subword methods.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://bheinzerling.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Benjamin Heinzerling is a postdoctoral researcher at RIKEN AIP and Tohoku University in Sendai, Japan. His research interests include analysis of entities mentioned in text, linking texts and knowledge bases, and multilingual subword methods.","tags":null,"title":"Benjamin Heinzerling","type":"authors"},{"authors":["Benjamin Heinzerling","Kentaro Inui"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"e2faf9815180499239b5cffc9713a23c","permalink":"https://bheinzerling.github.io/publication/heinzerling-2021-language/","publishdate":"2021-03-16T09:22:39.46039Z","relpermalink":"/publication/heinzerling-2021-language/","section":"publication","summary":"","tags":null,"title":"Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries","type":"publication"},{"authors":["Benjamin Heinzerling"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"70f7a0ae11e30f6279e40b1cecd9c8fb","permalink":"https://bheinzerling.github.io/thegradient.pub/nlps-clever-hans-moment-has-arrived/","publishdate":"2021-03-16T09:22:39.463419Z","relpermalink":"/thegradient.pub/nlps-clever-hans-moment-has-arrived/","section":"publication","summary":"","tags":null,"title":"NLP's Clever Hans Moment has Arrived","type":"publication"},{"authors":["Yi Zhu","Benjamin Heinzerling","Ivan Vulić","Michael Strube","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"b6339ac8469624dd0a8735f5d225a535","permalink":"https://bheinzerling.github.io/publication/zhu-2019-subword/","publishdate":"2019-11-08T07:36:28.992663Z","relpermalink":"/publication/zhu-2019-subword/","section":"publication","summary":"Recent work has validated the importance of subword information for word representation learning. Since subwords increase parameter sharing ability in neural models, their value should be even more pronounced in low-data regimes. In this work, we therefore provide a comprehensive analysis focused on the usefulness of subwords for word representation learning in truly low-resource scenarios and for three representative morphological tasks: fine-grained entity typing, morphological tagging, and named entity recognition. We conduct a systematic study that spans several dimensions of comparison: 1) type of data scarcity which can stem from the lack of task-specific training data, or even from the lack of unannotated data required to train word embeddings, or both; 2) language type by working with a sample of 16 typologically diverse languages including some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu); 3) the choice of the subword-informed word representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and task-based models, where having sufficient in-task data is a more critical requirement.","tags":null,"title":"On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages","type":"publication"},{"authors":["Pride Kavumba","Naoya Inoue","Benjamin Heinzerling","Keshav Singh","Paul Reisert","Kentaro Inui"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"92624f9808b6a4fdba17e4cc6d5b03dc","permalink":"https://bheinzerling.github.io/publication/kavumba-2019-choosing/","publishdate":"2019-11-08T07:36:15.506276Z","relpermalink":"/publication/kavumba-2019-choosing/","section":"publication","summary":"Pretrained language models, such as BERT and RoBERTa, have shown large improvements in the commonsense reasoning benchmark COPA. However, recent work found that many improvements in benchmarks of natural language understanding are not due to models learning the task, but due to their increasing ability to exploit superficial cues, such as tokens that occur more often in the correct answer than the wrong one. Are BERT′s and RoBERTa′s good performance on COPA also caused by this? We find superficial cues in COPA, as well as evidence that BERT exploits these cues.To remedy this problem, we introduce Balanced COPA, an extension of COPA that does not suffer from easy-to-exploit single token cues. We analyze BERT′s and RoBERTa′s performance on original and Balanced COPA, finding that BERT relies on superficial cues when they are present, but still achieves comparable performance once they are made ineffective, suggesting that BERT learns the task to a certain degree when forced to. In contrast, RoBERTa does not appear to rely on superficial cues.","tags":null,"title":"When Choosing Plausible Alternatives, Clever Hans can be Clever","type":"publication"},{"authors":["Federico López","Benjamin Heinzerling","Michael Strube"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"893bfd4bfbd371a6a78bcc0205b25110","permalink":"https://bheinzerling.github.io/publication/lopez-2019-finegrained/","publishdate":"2019-08-02T17:06:50.185526Z","relpermalink":"/publication/lopez-2019-finegrained/","section":"publication","summary":"How can we represent hierarchical information present in large type inventories for entity typing? We study the suitability of hyperbolic embeddings to capture hierarchical relations between mentions in context and their target types in a shared vector space. We evaluate on two datasets and propose two different techniques to extract hierarchical information from the type inventory: from an expert-generated ontology and by automatically mining the dataset. The hyperbolic model shows improvements in some but not all cases over its Euclidean counterpart. Our analysis suggests that the adequacy of this geometry depends on the granularity of the type inventory and the representation of its distribution.","tags":null,"title":"Fine-Grained Entity Typing in Hyperbolic Space","type":"publication"},{"authors":["Benjamin Heinzerling"],"categories":[],"content":"tl;dr: While the problem has been known for some time already, Niven \u0026amp; Kao present quite an egregious case of a neural network not learning what we think it learns.\nThe paper will appear at ACL 2019, preprint available here.\n It is now almost a cliché to find out that BERT (Devlin et al., 2019) performs \u0026ldquo;surprisingly well\u0026rdquo; on whatever dataset you throw at it. In this paper, Niven \u0026amp; Kao throw an argument comprehension dataset and, as expected, were surprised to find that with random choice giving 50 percent accuracy, a knowledge-rich model getting 61 percent, and the previously best model achieving 71 percent by pre-training on a larger dataset for a related task, finetuned BERT wins hands down, getting 77 percent right.\nSo far, so BERT, but here comes the twist: Instead of submitting yet another \u0026ldquo;we haz SOTA, accept plox\u0026rdquo; type paper, the authors were suspicious of this seemingly great success. Argument comprehension is a rather difficult task that requires world knowledge and commonsense reasoning (see Figure 1), and while no one doubts that BERT is one of the best language models created yet, there is little evidence that language models are capable of such feats of high-level natural language understanding.\n The Argument Reasoning Comprehension Task (Habernal et al., 2017, image source: Niven \u0026amp; Kao, 2019). Assuming a claim is made based on a given reason, select the piece of world knowledge (warrant or the alternative) that makes the claim valid. Here, the argument Google is not a harmful monopoly because people can choose not to use Google is valid \u0026ndash; or warranted in Toulmin\u0026rsquo;s terms \u0026ndash; if other search engines don\u0026rsquo;t redirect to Google, but invalid if all other search engines redirect to Google, because in the latter case users are forced to use Google, making Google a harmful monopoly. Sidenote: I think this example chosen by the authors is a bit unfortunate since it hinges on the strange use of redirect to meaning something like use search results provided by.   The authors perform three analyses. First, they count unigrams and bigrams in the possible answers (i.e. warrants) and observe that the presence of a single unigram like not, is, or do predicts the correct warrant better than random chance, indicating that such cues are useful and probably exploited by the model. Then, to check if the model indeed exploits such cues, the authors provide the model only with partial input, which makes reasoning about the correct answer impossible: For example, it should not be possible to reason about whether other search engines don\u0026rsquo;t redirect to Google or all other search engines redirect to Google is the correct warrant if no claim or reason is given. However, the model doesn\u0026rsquo;t care about this impossibility and identifies the correct warrant with 71 percent accuracy. After running similar experiments for the other two task-breaking settings (claim and warrant only; reason and warrant only), the authors conclude that dataset contains statistical cues and that BERT\u0026rsquo;s performance on this task can be entirely explained by its ability to exploit these cues. To drive the point home, in their third experiment the authors construct a version of the dataset in which the cues are not informative anymore and find that performance drops to random chance level. Without getting into a Chinese Room argument about what it means to understand something, most people would probably agree that a model making predictions based on the presence or absence of a handful of words like \u0026ldquo;not\u0026rdquo;, \u0026ldquo;is\u0026rdquo;, or \u0026ldquo;do\u0026rdquo; does not understand anything about argumentation. The authors declare that their SOTA result is meaningless.\n Our main finding is that these results are not meaningful and should be discarded.\n Of course, the problem of learners solving a task by learning the \u0026ldquo;wrong\u0026rdquo; thing has been known for a long time and is known as the Clever Hans effect, after the eponymous horse which appeared to be able to perform simple intellectual tasks, but in reality relied on involuntary cues given by its handler. Since the 1960s, versions of the tank anecdote tell of a neural network trained by the military to recognize tanks in images, but actually learning to recognize different levels of brightness due to one type of tank appearing only in bright photos and another type only in darker ones. Less anecdotal, Viktoria Krakovna has collected a depressingly long list of agents following the letter, but not the spirit of their reward function, with such gems as a video game agent learning to die at the end of the first level, since repeating that easy level gives a higher score than dying early in the harder second level. Two more recent, but already infamous cases are an image classifier claimed to be able to distinguish faces of criminals from those of law-abiding citizens, but actually recognizing smiles and a supposed \u0026ldquo;sexual orientation detector\u0026rdquo; which can be better explained as a detector of glasses, beards and eyeshadow.\nIf NLP is following in the footsteps of computer vision, it seems to be doomed to repeat its failures, too. Coming back to the paper, the authors point to a (again, depressingly) large amount of recent work reporting Clever Hans effects in NLP datasets. Especially notable among related work is McCoy, Pavlick \u0026amp; Linzen\u0026rsquo;s Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference, which finds an instance of the Clever Hans effect in NLI, and comes to the same conclusions:\n Statistical learners such as standard neural network architectures are prone to adopting shallow heuristics that succeed for the majority of training examples, instead of learning the underlying generalizations that they are intended to capture.\n For a broader view on this topic, also see Ana Marasović\u0026rsquo;s article on NLP\u0026rsquo;s Generalization Problem. To be clear, no one is claiming that large models like BERT or deep learning in general are useless (and I\u0026rsquo;ve found quite the opposite in my own work), but failures like the ones demonstrated in the paper and related work should make us skeptical about reports of near-human performance in high-level natural language understanding tasks.\nTwo minor gripes I have about the paper concern terminology. The authors call their second analysis \u0026ldquo;probing experiments\u0026rdquo;, but probing usually refers to training shallow classifiers on top of a deep neural network in order to determine what kind of information its representations contain, which is not done here. Similarly, the authors call the dataset they construct in their third analysis \u0026ldquo;adversarial dataset\u0026rdquo;, but adversarial usually refers to instances that mislead a model into making a wrong decision, which, again, is not done here. But these wording issues do not diminish the main findings of the paper.\nIn summary, Niven \u0026amp; Kao present a textbook case of the Clever Hans effect in NLP and remind us that as we train increasingly stronger learners, we need to pay increased attention to their ability of exploiting cues and taking unintended shortcuts.\nWhat does the Clever Hans effect mean for NLP? The growing number of papers finding cases of the Clever Hans effect raises important questions for NLP research, the most obvious one being how the effect can be prevented. When patterns in the dataset are aligned with the goal of the task at hand, a strong learner being able to recognize, remember, and generalize these patterns is desirable. But if the patterns are not what we\u0026rsquo;re actually interested in, then they become cues and shortcuts that allow the model to perform well without understanding the task. To prevent the Clever Hans effect, we hence need to aim for datasets without spurious patterns, and we need to assume that a well-performing model didn\u0026rsquo;t learn anything useful until proven otherwise. I\u0026rsquo;ll make suggestions for improving datasets first, then one for improving models.\nDatasets need more love Coming up with a model and improving it gives instant gratification from seeing the score go up during development, and SOTA on a common dataset all but ensures paper acceptance and ensuing citations. The gratification for creating a dataset is much more delayed and much less certain. Anecdotally, the default *ACL conference reviewer stance towards a paper proposing a novel1 model getting SOTA2 seems to be \u0026ldquo;accept\u0026rdquo;, while a paper introducing a new dataset needs to fight against a \u0026ldquo;this paper only introduces a new dataset -\u0026gt; reject\u0026rdquo; attitude: People who create datasets are not doing real science, and while they\u0026rsquo;re free to have their own little conference in exotic locations, they obviously are not smart enough to import tensorflow as tf, so they shouldn\u0026rsquo;t pollute top tier conferences with their boring resource papers. This post by Rachel Bawden gives examples of this kind of reviewer attitude.\nOf course, not all resource papers can be published at top conferences with low acceptance rates, but if we have to choose between having too many models or too many datasets, I\u0026rsquo;d argue more datasets will have a more solid and lasting positive impact. Better acceptance prospects will encourage more researchers to create datasets, which will give us more and (hopefully) better datasets, which, in turn, will allow overcoming the current dataset monoculture with one \u0026ldquo;standard\u0026rdquo; dataset per task, and the resulting dataset diversity will, finally, allow more robust evaluation of models.\nDataset ablations and public betas Ablating, i.e. removing, part of a model and observing the impact this has on performance is a common method for verifying that the part in question is useful. If performance doesn\u0026rsquo;t go down, then the part is useless and should be removed. Carrying this method over to datasets, it should become common practice to perform dataset ablations, as well, for example:\n Provide only incomplete input (as done in the reviewed paper): This verifies that the complete input is required. If not, the dataset contains cues that allow taking shortcuts. Shuffle the input: This verifies the importance of word (or sentence) order. If a bag-of-words/sentences gives similar results, even though the task requires sequential reasoning, then the model has not learned sequential reasoning and the dataset contains cues that allow the model to \u0026ldquo;solve\u0026rdquo; the task without it. Assign random labels: How much does performance drop if ten percent of instances are relabeled randomly? How much with all random labels? If scores don\u0026rsquo;t change much, the model probably didn\u0026rsquo;t learning anything interesting about the task. Randomly replace content words: How much does performance drop if all noun phrases and/or verb phrases are replaced with random noun phrases and verbs? If not much, the dataset may provide unintended non-content cues, such as sentence length or distribution of function words.  New datasets could be improved and verified by initially treating them as being in a public beta phase, in which claims are made only in terms of performance on dataset X and not in terms of performance in task Y. Model creators using a dataset in public beta would also have to perform dataset ablations until we can be reasonably sure that the dataset contains no simple cues or shortcuts.\nInter-prediction agreement If, for example, adding an unrelated sentence to the input causes a question-answering model to give a different answer (see Figure 2), the model is not really understanding the question. Rather, it seems to have learned heuristics like if the question contains the words \u0026ldquo;what\u0026rdquo; and \u0026ldquo;name\u0026rdquo; the answer is the first proper noun phrase in the last sentence.\n A question-answering model being fooled by a meaning-preserving addition of an unrelated sentence, shown in blue (Source: Jia and Liang, 2017).   Failures like these mean that we should not only aim to create better models that co-evolve with increasingly difficult datasets, as proposed by Zellers et al. (2019), but also improve models by making them more robust. Towards this goal, model creators need to adopt a Build It, Break It mentality, according to which it is not enough to get a high score on a certain dataset, but also necessary to test a model\u0026rsquo;s robustness. Such tests could be done by something akin to fuzzing in software testing, where an attacker attempts to find inputs that cause a system to fail.\nIf dataset creators have to report inter-annotator agreement to allow judging the consistency of annotations, we should require model creators to report inter-prediction agreement to allow judging the consistency of model predictions. Such a score could be calculated on sets of semantically equivalent instances, e.g. as the proportion of predictions that remain the same under meaning-preserving perturbations of a test instance.\n  The importance of being novel cannot be overstated. A paper proposing a model that is merely new has little chance of being accepted. \u0026#x21a9;\u0026#xfe0e;\n Also see Anna Rogers\u0026rsquo; critique of the SOTA-centric approach to NLP. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1563719959,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563719959,"objectID":"12ca69a6cfff445531a26d82be1a516b","permalink":"https://bheinzerling.github.io/post/clever-hans/","publishdate":"2019-07-21T23:39:19+09:00","relpermalink":"/post/clever-hans/","section":"post","summary":"A review of Timothy Niven and Hung-Yu Kao, 2019: Probing Neural Network Comprehension of Natural Language Arguments.","tags":[],"title":"NLP's Clever Hans Moment has Arrived","type":"post"},{"authors":["Benjamin Heinzerling","Michael Strube"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"312fa04ce83edb9e13e3db5d3e14eade","permalink":"https://bheinzerling.github.io/publication/heinzerling-2019-sequence/","publishdate":"2019-07-23T03:48:44.369816Z","relpermalink":"/publication/heinzerling-2019-sequence/","section":"publication","summary":"","tags":null,"title":"Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation","type":"publication"},{"authors":["Benjamin Heinzerling","Michael Strube"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"ebeb3b2818b288ce3b9684607660a102","permalink":"https://bheinzerling.github.io/publication/heinzerling-2018-bpemb/","publishdate":"2019-07-23T03:48:44.372067Z","relpermalink":"/publication/heinzerling-2018-bpemb/","section":"publication","summary":"","tags":null,"title":"BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages","type":"publication"},{"authors":["Markus Zopf","Teresa Botschen","Tobias Falke","Beniamin Heinzerling","Ana Marasovic","Todor Mihaylov","PVS Avinesh","Eneldo Loza Mencia","Johannes Fürnkranz","Anette Frank"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"2df08c20e8cf21cc9f990d6b5480f653","permalink":"https://bheinzerling.github.io/publication/zopf-2018-important/","publishdate":"2019-07-23T03:48:44.371234Z","relpermalink":"/publication/zopf-2018-important/","section":"publication","summary":"","tags":null,"title":"What's important in a text? An extensive evaluation of linguistic annotations for summarization","type":"publication"},{"authors":["Benjamin Heinzerling","Nafise Sadat Moosavi","Michael Strube"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"4bfd8a11e36d668360e446d7a5145d32","permalink":"https://bheinzerling.github.io/publication/heinzerling-2017-selectional/","publishdate":"2020-07-08T09:05:33.013719Z","relpermalink":"/publication/heinzerling-2017-selectional/","section":"publication","summary":"","tags":null,"title":"Revisiting Selectional Preferences for Coreference Resolution","type":"publication"},{"authors":["Benjamin Heinzerling","Michael Strube","Chin-Yew Lin"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"62424200ad7e67513ea856654857d7bd","permalink":"https://bheinzerling.github.io/publication/heinzerling-2017-trust/","publishdate":"2019-07-23T03:48:44.373606Z","relpermalink":"/publication/heinzerling-2017-trust/","section":"publication","summary":"","tags":null,"title":"Trust, but Verify! Better Entity Linking through Automatic Verification","type":"publication"},{"authors":["Benjamin Heinzerling","Michael Strube"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"78cf7557f4cf9242cfa7860ff6a200c2","permalink":"https://bheinzerling.github.io/publication/heinzerling-2015-visual/","publishdate":"2019-07-23T03:48:44.375044Z","relpermalink":"/publication/heinzerling-2015-visual/","section":"publication","summary":"","tags":null,"title":"Visual Error Analysis for Entity Linking","type":"publication"},{"authors":["Benjamin Heinzerling","Alex Judea","Michael Strube"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"f7d0520547a586f125be730947b5b546","permalink":"https://bheinzerling.github.io/publication/heinzerling-2015-tac/","publishdate":"2019-07-23T03:48:44.374392Z","relpermalink":"/publication/heinzerling-2015-tac/","section":"publication","summary":"","tags":null,"title":"HITS at TAC KBP 2015: Entity discovery and linking, and event nugget detection","type":"publication"},{"authors":[" Alex Judea","Benjamin Heinzerling","Michael Strube"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"c9523db6f84e76b143ae829195e97480","permalink":"https://bheinzerling.github.io/publication/judea-2014-tac/","publishdate":"2019-07-23T03:48:44.375751Z","relpermalink":"/publication/judea-2014-tac/","section":"publication","summary":"","tags":null,"title":"HITS' Monolingual and Cross-lingual Entity Linking System at TAC 2014","type":"publication"},{"authors":["Angela Fahrni","Benjamin Heinzerling","Thierry Göckel","Michael Strube"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"2f2900f079d36334030e7e32f54b4087","permalink":"https://bheinzerling.github.io/publication/fahrni-2013-tac/","publishdate":"2019-07-23T03:48:44.3763Z","relpermalink":"/publication/fahrni-2013-tac/","section":"publication","summary":"","tags":null,"title":"HITS' Monolingual and Cross-lingual Entity Linking System at TAC 2013","type":"publication"}]